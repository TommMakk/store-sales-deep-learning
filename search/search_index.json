{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Store Sales - Deep Learning Solution","text":"<p>Welcome to the documentation for the Store Sales - Deep Learning Solution project. This project provides a reproducible pipeline for forecasting store sales using deep learning, developed for the Kaggle Store Sales - Time Series Forecasting competition.</p>"},{"location":"#overview","title":"Overview","text":"<p>The goal of this project is to predict daily sales for a large Ecuadorian grocery retailer using historical sales data, promotions, oil prices, holidays, and store information. The solution leverages modern data science best practices, modular code, and advanced deep learning techniques.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated data processing and feature engineering pipelines</li> <li>Deep learning model built with Keras/TensorFlow</li> <li>Support for reproducible experiments and easy extensibility</li> <li>Modular project structure following Cookiecutter Data Science principles</li> </ul>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<ul> <li>Getting Started: How to set up and run the project</li> <li>Data Preparation: Steps for processing raw data</li> <li>Feature Engineering: Creating features for modeling</li> <li>Model Training: Training the deep learning model</li> <li>Model Inference: Generating predictions</li> <li>Deep Learning Approach: Details of the modeling methodology</li> <li>Project Structure: Explanation of the codebase layout</li> <li>Future Improvements: Ideas for further development</li> <li>References: Useful links and resources</li> </ul> <p>For more details on each step, use the navigation menu or follow the links above.</p>"},{"location":"data_preparation/","title":"Data Preparation","text":"<p>This section describes the steps taken to prepare the data for the Store Sales - Deep Learning Solution project.</p>"},{"location":"data_preparation/#1-raw-data-collection","title":"1. Raw Data Collection","text":"<ul> <li>Download the competition data from Kaggle Store Sales - Time Series Forecasting.</li> <li>Place all raw CSV files in the <code>data/raw/</code> directory.</li> </ul>"},{"location":"data_preparation/#2-data-processing-pipeline","title":"2. Data Processing Pipeline","text":"<p>The data processing pipeline is responsible for: - Loading raw data files (sales, stores, oil, holidays, etc.). - Merging datasets to create a unified view. - Handling missing values and correcting data types. - Saving interim datasets to <code>data/interim/</code> for further processing.</p> <p>You can run the data processing pipeline with:</p> <p>python store-sales-DL/dataset.py</p> <p>or, using the Makefile:</p> <p>make dataset</p>"},{"location":"data_preparation/#3-output","title":"3. Output","text":"<ul> <li>The processed interim datasets are saved in the <code>data/interim/</code> directory.</li> <li>These datasets are used as input for the feature engineering step.</li> </ul>"},{"location":"data_preparation/#4-notes","title":"4. Notes","text":"<ul> <li>Ensure all raw data files are present and named correctly before running the pipeline.</li> <li>Review the logs/output for any warnings or errors during processing.</li> </ul> <p>For more details on feature engineering, see the Feature Engineering section.</p>"},{"location":"deep_learning/","title":"Deep Learning Approach","text":"<p>This section outlines the deep learning methodology used for the Store Sales - Time Series Forecasting project.</p>"},{"location":"deep_learning/#methodology","title":"Methodology","text":"<ul> <li> <p>Data Preprocessing &amp; Cleaning:   Raw sales, oil, holiday, and store data are merged, missing values are handled, and categorical variables are encoded using one-hot encoding.</p> </li> <li> <p>Feature Engineering:   Lag-based features, rolling statistics, and calendar-based features (such as day of week, holidays, and paydays) are generated to capture temporal patterns and external influences on sales.</p> </li> <li> <p>Exploratory Data Analysis (EDA):   Data distributions, trends, and correlations are visualized to inform feature selection and model design.</p> </li> <li> <p>Model Development:   The primary model is a feedforward neural network implemented in Keras/TensorFlow. The architecture consists of multiple dense layers with ReLU activations and dropout regularization to prevent overfitting.</p> </li> <li> <p>Training Strategy:   The Adam optimizer is used with a learning rate of 0.001. Early stopping is employed to halt training when validation loss stops improving, ensuring optimal generalization.</p> </li> <li> <p>Evaluation:   Model performance is monitored using Mean Squared Error (MSE) and Mean Absolute Error (MAE) on a validation set. The final model is saved and used for inference on the test set.</p> </li> </ul>"},{"location":"deep_learning/#model-architecture","title":"Model Architecture","text":"<ul> <li>Input Layer: Accepts engineered features for each store-date-family combination.</li> <li>Hidden Layers: Two dense layers (96 and 64 units) with ReLU activation and dropout.</li> <li>Output Layer: Single neuron with linear activation for sales prediction.</li> </ul>"},{"location":"deep_learning/#training-and-optimization","title":"Training and Optimization","text":"<ul> <li>Optimizer: Adam (learning rate = 0.001)</li> <li>Loss Function: Mean Squared Error (MSE)</li> <li>Batch Size: 64</li> <li>Epochs: Up to 20, with early stopping (patience = 4)</li> <li>Regularization: Dropout (rate = 0.1) to reduce overfitting</li> </ul>"},{"location":"deep_learning/#reproducibility-automation","title":"Reproducibility &amp; Automation","text":"<ul> <li>All steps, from data preparation to model training and inference, are automated via modular scripts.</li> <li>Model checkpoints and logs are saved for transparency and reproducibility.</li> </ul> <p>This deep learning approach is designed to capture both short-term and long-term sales patterns, leveraging rich feature engineering and modern neural network techniques to deliver accurate forecasts for the Kaggle Store Sales competition.</p>"},{"location":"feature_engineering/","title":"Feature Engineering","text":"<p>This section describes the feature engineering process used to enhance the predictive power of the deep learning model for store sales forecasting.</p>"},{"location":"feature_engineering/#overview","title":"Overview","text":"<p>Feature engineering transforms raw data into meaningful inputs for the model. In this project, several types of features were created to capture temporal patterns, external influences, and categorical relationships.</p>"},{"location":"feature_engineering/#types-of-features","title":"Types of Features","text":"<ul> <li> <p>Lag Features:   Previous sales values (e.g., sales from 1, 7, 14, or 28 days ago) are included to help the model recognize trends and seasonality.</p> </li> <li> <p>Rolling Statistics:   Rolling means and standard deviations over various windows (e.g., 7-day, 14-day) provide information about recent sales trends and volatility.</p> </li> <li> <p>Promotions:   Binary indicators for whether a product was on promotion, as well as rolling counts of recent promotions, are included to capture the impact of marketing activities.</p> </li> <li> <p>Calendar Features:   Day of week, month, year, holidays, and paydays are encoded to help the model learn periodic patterns and the effects of special dates.</p> </li> <li> <p>Store and Product Metadata:   Store type, location, and product family are encoded using one-hot or label encoding to provide context about each sale.</p> </li> <li> <p>External Data:   Oil prices and holiday events are merged with the main dataset to account for macroeconomic and external factors.</p> </li> </ul>"},{"location":"feature_engineering/#feature-engineering-pipeline","title":"Feature Engineering Pipeline","text":"<p>The feature engineering process is automated in the <code>features.py</code> script. This script:</p> <ul> <li>Loads interim datasets from the data processing step.</li> <li>Creates and merges all engineered features.</li> <li>Handles missing values and encodes categorical variables.</li> <li>Outputs the final processed datasets to <code>data/processed/</code> for model training and evaluation.</li> </ul> <p>You can run the feature engineering pipeline with:</p> <p>python store-sales-DL/features.py</p> <p>or, using the Makefile:</p> <p>make features</p> <p>Well-designed features are crucial for improving model accuracy and capturing the complex dynamics of retail sales.</p>"},{"location":"future_improvements/","title":"Future Improvements","text":"<p>While the current deep learning approach provides strong predictive performance for store sales forecasting, there are several avenues for further improvement:</p>"},{"location":"future_improvements/#advanced-model-architectures","title":"Advanced Model Architectures","text":"<ul> <li>Recurrent Neural Networks (RNNs):   Implement RNNs, such as LSTM or GRU layers, to better capture sequential dependencies and long-term trends in time-series data.</li> <li>Temporal Convolutional Networks (TCNs):   Explore TCNs for their ability to model temporal relationships with convolutional layers.</li> <li>Attention Mechanisms:   Integrate attention layers to help the model focus on the most relevant time steps or features.</li> </ul>"},{"location":"future_improvements/#hybrid-and-ensemble-models","title":"Hybrid and Ensemble Models","text":"<ul> <li>Hybrid Approaches:   Combine deep learning models with tree-based methods (e.g., XGBoost, LightGBM) or statistical models (e.g., ARIMA, Prophet) to leverage the strengths of each.</li> <li>Ensembling:   Blend predictions from multiple models to reduce variance and improve robustness.</li> </ul>"},{"location":"future_improvements/#feature-enrichment","title":"Feature Enrichment","text":"<ul> <li>External Data Sources:   Incorporate additional data such as weather, economic indicators, or local events to provide more context for sales fluctuations.</li> <li>Automated Feature Selection:   Use feature selection techniques or embedding layers to identify and utilize the most informative features.</li> </ul>"},{"location":"future_improvements/#training-and-evaluation-enhancements","title":"Training and Evaluation Enhancements","text":"<ul> <li>Hyperparameter Optimization:   Apply automated tools like Optuna or Keras Tuner for more thorough hyperparameter search.</li> <li>Cross-Validation:   Implement time-series cross-validation (e.g., rolling or expanding window) to better estimate model generalization and avoid overfitting.</li> </ul>"},{"location":"future_improvements/#model-interpretability","title":"Model Interpretability","text":"<ul> <li>Explainability Tools:   Use SHAP, LIME, or similar tools to interpret model predictions and understand feature importance, aiding business decision-making.</li> </ul>"},{"location":"future_improvements/#model-selection-for-time-series","title":"Model Selection for Time-Series","text":"<p>While deep learning models are powerful and flexible, they can be overkill for many time-series forecasting problems, especially when data is limited or patterns are well-captured by simpler models. Classical approaches such as ARIMA, SARIMA, Exponential Smoothing, or tree-based models like XGBoost often provide competitive or superior results with less computational overhead and easier interpretability. For many business applications, these models may be preferable unless the dataset is large, highly complex, or contains significant nonlinearities that deep learning can uniquely exploit.</p> <p>Continued experimentation and innovation in these areas can further improve forecasting accuracy and business value.</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This section provides step-by-step instructions to set up the Store Sales - Deep Learning Solution project on your local machine.</p>"},{"location":"getting_started/#1-clone-the-repository","title":"1. Clone the Repository","text":"<p>Clone the project from GitHub:</p> <p>git clone https://github.com/yourusername/store-sales-DL.git cd store-sales-DL</p>"},{"location":"getting_started/#2-set-up-a-python-environment","title":"2. Set Up a Python Environment","text":"<p>It is recommended to use a virtual environment:</p> <p>python3 -m venv venv source venv/bin/activate</p>"},{"location":"getting_started/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>Install the required Python packages:</p> <p>pip install -r requirements.txt</p>"},{"location":"getting_started/#4-download-the-data","title":"4. Download the Data","text":"<ul> <li>Register and download the competition data from Kaggle Store Sales - Time Series Forecasting.</li> <li>Place all raw CSV files in the <code>data/raw/</code> directory.</li> </ul>"},{"location":"getting_started/#5-run-the-pipeline","title":"5. Run the Pipeline","text":"<p>You can run each step of the pipeline using Python scripts or the Makefile:</p> <ul> <li> <p>Data Preparation:   python store-sales-DL/dataset.py   or   make dataset</p> </li> <li> <p>Feature Engineering:   python store-sales-DL/features.py   or   make features</p> </li> <li> <p>Model Training:   python store-sales-DL/modeling/train.py   or   make train</p> </li> <li> <p>Model Inference:   python store-sales-DL/modeling/predict.py   or   make predict</p> </li> </ul> <p>For more details on each step, refer to the corresponding sections in this documentation.</p>"},{"location":"model_inference/","title":"Model Inference","text":"<p>This section explains how to use the trained deep learning model to generate sales predictions for the Store Sales - Deep Learning Solution project.</p>"},{"location":"model_inference/#overview","title":"Overview","text":"<p>Model inference involves loading the best trained model and applying it to new or unseen data (such as the test set) to generate sales forecasts. This step is essential for evaluating model performance and submitting predictions to the competition.</p>"},{"location":"model_inference/#steps-for-model-inference","title":"Steps for Model Inference","text":"<ol> <li> <p>Prepare the Test Data</p> </li> <li> <p>Ensure that the test data has been processed and features have been engineered in the same way as the training data.</p> </li> <li> <p>The processed test dataset should be located in the appropriate directory (e.g., <code>data/processed/</code>).</p> </li> <li> <p>Run the Inference Script</p> </li> </ol> <p>You can generate predictions using the provided script:</p> <p>python store-sales-DL/modeling/predict.py</p> <p>or, using the Makefile:</p> <p>make predict</p> <ol> <li> <p>Output</p> </li> <li> <p>The script will load the best saved model (e.g., <code>models/best_model.keras</code>).</p> </li> <li>Predictions for the test set will be generated and saved, typically as a CSV file in the <code>models/</code> or <code>data/processed/</code> directory.</li> <li>The output file can be used for submission to Kaggle or further analysis.</li> </ol>"},{"location":"model_inference/#notes","title":"Notes","text":"<ul> <li>Ensure that the model and test data paths in the script match your project structure.</li> <li>Review the logs/output for any errors or warnings during inference.</li> <li>You can modify the inference script to adjust output format or post-processing as needed.</li> </ul> <p>For more details on model training, see the Model Training section.</p>"},{"location":"model_training/","title":"Model Training","text":"<p>This section describes the process of training the deep learning model for the Store Sales - Deep Learning Solution project.</p>"},{"location":"model_training/#overview","title":"Overview","text":"<p>Model training involves fitting a neural network to the processed and feature-engineered sales data. The goal is to learn patterns and relationships that enable accurate sales forecasting.</p>"},{"location":"model_training/#steps-for-model-training","title":"Steps for Model Training","text":"<ol> <li> <p>Prepare the Training and Validation Data</p> </li> <li> <p>Ensure that the processed datasets (including features and targets) are available in the <code>data/processed/</code> directory.</p> </li> <li> <p>The data should be split into training and validation sets to monitor model performance and prevent overfitting.</p> </li> <li> <p>Run the Training Script</p> </li> </ol> <p>You can train the model using the provided script:</p> <p>python store-sales-DL/modeling/train.py</p> <p>or, using the Makefile:</p> <p>make train</p> <ol> <li> <p>Training Details</p> </li> <li> <p>The script loads the processed data, builds the neural network, and trains it using the specified hyperparameters.</p> </li> <li>Early stopping is used to halt training when validation loss stops improving.</li> <li> <p>Model checkpoints and logs are saved for transparency and reproducibility.</p> </li> <li> <p>Output</p> </li> <li> <p>The best model is saved (e.g., as <code>models/best_model.keras</code>) for later inference.</p> </li> <li>Training and validation metrics are displayed and can be logged for further analysis.</li> </ol>"},{"location":"model_training/#notes","title":"Notes","text":"<ul> <li>You can adjust hyperparameters (e.g., learning rate, batch size, number of epochs) in the training script to experiment with different configurations.</li> <li>Ensure that the paths to data and model directories are correct in your environment.</li> <li>For best results, monitor validation metrics to avoid overfitting.</li> </ul> <p>For more details on model inference, see the Model Inference section.</p>"},{"location":"project_structure/","title":"Project Structure","text":"<p>This section describes the organization of the Store Sales - Deep Learning Solution project. The structure follows best practices for reproducibility, modularity, and scalability, inspired by the Cookiecutter Data Science template.</p>"},{"location":"project_structure/#directory-layout","title":"Directory Layout","text":"<pre><code>\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 [README.md](http://_vscodecontentref_/0)\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 external       &lt;- Data from third party sources.\n\u2502   \u251c\u2500\u2500 interim        &lt;- Intermediate data that has been transformed.\n\u2502   \u251c\u2500\u2500 processed      &lt;- The final, canonical data sets for modeling.\n\u2502   \u2514\u2500\u2500 raw            &lt;- The original, immutable data dump.\n\u2502\n\u251c\u2500\u2500 docs               &lt;- Documentation and project reports.\n\u251c\u2500\u2500 models             &lt;- Trained and serialized models, model predictions, or model summaries.\n\u251c\u2500\u2500 notebooks          &lt;- Jupyter notebooks for exploration and analysis.\n\u251c\u2500\u2500 [pyproject.toml](http://_vscodecontentref_/1)     &lt;- Project configuration and metadata.\n\u251c\u2500\u2500 references         &lt;- Data dictionaries, manuals, and explanatory materials.\n\u251c\u2500\u2500 reports            &lt;- Generated analysis as HTML, PDF, LaTeX, etc.\n\u2502   \u2514\u2500\u2500 figures        &lt;- Generated graphics and figures for reporting.\n\u251c\u2500\u2500 [requirements.txt](http://_vscodecontentref_/2)   &lt;- Python dependencies for the project.\n\u251c\u2500\u2500 [setup.cfg](http://_vscodecontentref_/3)          &lt;- Configuration for code style tools.\n\u2514\u2500\u2500 store_sales_DL     &lt;- Source code for this project.\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 config.py               &lt;- Project configuration variables.\n    \u251c\u2500\u2500 dataset.py              &lt;- Data loading and preprocessing scripts.\n    \u251c\u2500\u2500 features.py             &lt;- Feature engineering code.\n    \u251c\u2500\u2500 modeling\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 predict.py          &lt;- Model inference code.\n    \u2502   \u2514\u2500\u2500 train.py            &lt;- Model training code.\n    \u2514\u2500\u2500 plots.py                &lt;- Visualization code.\n</code></pre>"},{"location":"project_structure/#key-components","title":"Key Components","text":"<ul> <li>data/: Contains all data files, organized by processing stage.</li> <li>docs/: Project documentation and MkDocs files.</li> <li>models/: Saved models and prediction outputs.</li> <li>notebooks/: Jupyter notebooks for exploration and analysis.</li> <li>store_sales_DL/: Main source code for data processing, feature engineering, modeling, and visualization.</li> <li>Makefile: Automation commands for running the pipeline.</li> <li>requirements.txt: List of Python dependencies.</li> </ul> <p>This structure ensures clarity, maintainability, and ease of collaboration for both development and production workflows.</p>"},{"location":"references/","title":"References","text":"<p>Below are useful links and resources related to the Store Sales - Deep Learning Solution project and time series forecasting in general.</p>"},{"location":"references/#competition-and-data","title":"Competition and Data","text":"<ul> <li>Kaggle Store Sales - Time Series Forecasting Competition</li> <li>Kaggle Competition Data</li> </ul>"},{"location":"references/#project-structure-and-best-practices","title":"Project Structure and Best Practices","text":"<ul> <li>Cookiecutter Data Science</li> <li>Cookiecutter GitHub Repository</li> </ul>"},{"location":"references/#deep-learning-and-time-series","title":"Deep Learning and Time Series","text":"<ul> <li>TensorFlow Documentation</li> <li>Keras Documentation</li> <li>A Gentle Introduction to Time Series Forecasting with Deep Learning</li> </ul>"},{"location":"references/#python-tools","title":"Python Tools","text":"<ul> <li>pandas Documentation</li> <li>scikit-learn Documentation</li> <li>Matplotlib Documentation</li> </ul>"},{"location":"references/#other-useful-resources","title":"Other Useful Resources","text":"<ul> <li>Optuna Hyperparameter Optimization</li> <li>SHAP for Model Interpretability</li> <li>Kaggle Learn: Time Series</li> </ul>"}]}